{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scfLT2i0MLyD"
      },
      "source": [
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/rapidsai-community/showcase/blob/main/getting_started_tutorials/rapids-pip-colab-template.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Install RAPIDS into Colab\"/>\n",
        "</a>\n",
        "\n",
        "# RAPIDS cuDF is now already on your Colab instance!\n",
        "RAPIDS cuDF is preinstalled on Google Colab and instantly accelerates Pandas with zero code changes. [You can quickly get started with our tutorial notebook](https://nvda.ws/rapids-cudf). This notebook template is for users who want to utilize the full suite of the RAPIDS libraries for their workflows on Colab.  \n",
        "\n",
        "# Environment Sanity Check #\n",
        "\n",
        "Click the _Runtime_ dropdown at the top of the page, then _Change Runtime Type_ and confirm the instance type is _GPU_.\n",
        "\n",
        "You can check the output of `!nvidia-smi` to check which GPU you have.  Please uncomment the cell below if you'd like to do that.  Currently, RAPIDS runs on all available Colab GPU instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67T0090Jk2KL"
      },
      "source": [
        "# !nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_v33LnDVNo3"
      },
      "source": [
        "#Setup:\n",
        "This set up script:\n",
        "\n",
        "1. Checks to make sure that the GPU is RAPIDS compatible\n",
        "1. Pip Installs the RAPIDS' libraries, which are:\n",
        "  1. cuDF\n",
        "  1. cuML\n",
        "  1. cuGraph\n",
        "  1. cuSpatial\n",
        "  1. cuxFilter\n",
        "  1. cuCIM\n",
        "  1. xgboost\n",
        "\n",
        "# Controlling Which RAPIDS Version is Installed\n",
        "This line in the cell below, `!python rapidsai-csp-utils/colab/pip-install.py`, kicks off the RAPIDS installation script.  You can control the RAPIDS version installed by adding either `latest`, `nightlies` or the default/blank option.  Example:\n",
        "\n",
        "`!python rapidsai-csp-utils/colab/pip-install.py <option>`\n",
        "\n",
        "You can now tell the script to install:\n",
        "1. **RAPIDS + Colab Default Version**, by leaving the install script option blank (or giving an invalid option), adds the rest of the RAPIDS libraries to the RAPIDS cuDF library preinstalled on Colab.  **This is the default and recommended version.**  Example: `!python rapidsai-csp-utils/colab/pip-install.py`\n",
        "1. **Latest known working RAPIDS stable version**, by using the option `latest` upgrades all RAPIDS labraries to the latest working RAPIDS stable version.  Usually early access for future RAPIDS+Colab functionality - some functionality may not work, but can be same as the default version. Example: `!python rapidsai-csp-utils/colab/pip-install.py latest`\n",
        "1. **the current nightlies version**, by using the option, `nightlies`, installs current RAPIDS nightlies version.  For RAPIDS Developer use - **not recommended/untested**.  Example: `!python rapidsai-csp-utils/colab/pip-install.py nightlies`\n",
        "\n",
        "\n",
        "**This will complete in about 5-6 minutes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0C8IV5TQnjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83054d64-59d8-4ffe-a61e-b9d351f3813d"
      },
      "source": [
        "# This get the RAPIDS-Colab install files and test check your GPU.  Run this and the next cell only.\n",
        "# Please read the output of this cell.  If your Colab Instance is not RAPIDS compatible, it will warn you and give you remediation steps.\n",
        "!git clone https://github.com/rapidsai/rapidsai-csp-utils.git\n",
        "!python rapidsai-csp-utils/colab/pip-install.py\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'rapidsai-csp-utils'...\n",
            "remote: Enumerating objects: 592, done.\u001b[K\n",
            "remote: Counting objects: 100% (158/158), done.\u001b[K\n",
            "remote: Compressing objects: 100% (76/76), done.\u001b[K\n",
            "remote: Total 592 (delta 125), reused 82 (delta 82), pack-reused 434 (from 3)\u001b[K\n",
            "Receiving objects: 100% (592/592), 194.79 KiB | 21.64 MiB/s, done.\n",
            "Resolving deltas: 100% (299/299), done.\n",
            "Installing RAPIDS remaining 25.04 libraries\n",
            "Using Python 3.11.12 environment at: /usr\n",
            "Resolved 173 packages in 11.89s\n",
            "Downloading libcuml-cu12 (404.9MiB)\n",
            "Downloading cudf-cu12 (1.7MiB)\n",
            "Downloading cugraph-cu12 (3.0MiB)\n",
            "Downloading rmm-cu12 (1.5MiB)\n",
            "Downloading ucx-py-cu12 (2.2MiB)\n",
            "Downloading dask (1.3MiB)\n",
            "Downloading datashader (17.5MiB)\n",
            "Downloading bokeh (6.6MiB)\n",
            "Downloading libcuspatial-cu12 (31.1MiB)\n",
            "Downloading libcuvs-cu12 (1.1GiB)\n",
            "Downloading pylibcudf-cu12 (26.4MiB)\n",
            "Downloading librmm-cu12 (2.9MiB)\n",
            "Downloading libcudf-cu12 (538.8MiB)\n",
            "Downloading shapely (2.4MiB)\n",
            "Downloading libcugraph-cu12 (1.4GiB)\n",
            "Downloading raft-dask-cu12 (274.9MiB)\n",
            "Downloading cuspatial-cu12 (4.1MiB)\n",
            "Downloading cuml-cu12 (9.4MiB)\n",
            "Downloading pylibcugraph-cu12 (2.0MiB)\n",
            "Downloading cuproj-cu12 (1.1MiB)\n",
            "Downloading libkvikio-cu12 (2.0MiB)\n",
            "Downloading libraft-cu12 (20.8MiB)\n",
            "Downloading cucim-cu12 (5.6MiB)\n",
            "Downloading libcugraph-cu12 (1.4GiB)\n",
            "Downloading libraft-cu12 (20.8MiB)\n",
            "Downloading cuspatial-cu12 (4.1MiB)\n",
            "Downloading cuproj-cu12 (1.1MiB)\n",
            "Downloading cucim-cu12 (5.6MiB)\n",
            "Downloading raft-dask-cu12 (274.9MiB)\n",
            "Downloading ucx-py-cu12 (2.2MiB)\n",
            "Downloading cudf-cu12 (1.7MiB)\n",
            "Downloading rmm-cu12 (1.5MiB)\n",
            "Downloading pylibcugraph-cu12 (2.0MiB)\n",
            "Downloading cugraph-cu12 (3.0MiB)\n",
            "Downloading libcudf-cu12 (538.8MiB)\n",
            "Downloading pylibcudf-cu12 (26.4MiB)\n",
            "Downloading libkvikio-cu12 (2.0MiB)\n",
            "Downloading cuml-cu12 (9.4MiB)\n",
            "Downloading librmm-cu12 (2.9MiB)\n",
            " Downloaded cuproj-cu12\n",
            "Downloading libcuvs-cu12 (1.1GiB)\n",
            " Downloaded rmm-cu12\n",
            " Downloaded cudf-cu12\n",
            "Downloading libcuspatial-cu12 (31.1MiB)\n",
            " Downloaded libkvikio-cu12\n",
            " Downloaded shapely\n",
            "Downloading libcuml-cu12 (404.9MiB)\n",
            " Downloaded cugraph-cu12\n",
            " Downloaded datashader\n",
            " Downloaded dask\n",
            " Downloaded cucim-cu12\n",
            " Downloaded pylibcugraph-cu12\n",
            " Downloaded ucx-py-cu12\n",
            " Downloaded librmm-cu12\n",
            " Downloaded cuspatial-cu12\n",
            " Downloaded bokeh\n",
            " Downloaded pylibcudf-cu12\n",
            " Downloaded cuml-cu12\n",
            " Downloaded libraft-cu12\n",
            " Downloaded libcuspatial-cu12\n",
            " Downloaded raft-dask-cu12\n",
            " Downloaded libcuml-cu12\n",
            " Downloaded libcudf-cu12\n",
            " Downloaded libcuvs-cu12\n",
            " Downloaded libcugraph-cu12\n",
            "Prepared 52 packages in 29.59s\n",
            "Uninstalled 29 packages in 741ms\n",
            "Installed 52 packages in 102ms\n",
            " + arrow==1.3.0\n",
            " - bokeh==3.7.3\n",
            " + bokeh==3.6.3\n",
            " + cucim-cu12==25.4.0\n",
            " - cudf-cu12==25.2.1 (from https://pypi.nvidia.com/cudf-cu12/cudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl)\n",
            " + cudf-cu12==25.4.0\n",
            " + cugraph-cu12==25.4.1\n",
            " - cuml-cu12==25.2.1\n",
            " + cuml-cu12==25.4.0\n",
            " + cuproj-cu12==25.4.0\n",
            " + cuspatial-cu12==25.4.0\n",
            " - cuvs-cu12==25.2.1\n",
            " + cuvs-cu12==25.4.0\n",
            " + cuxfilter-cu12==25.4.0\n",
            " - dask==2024.12.1\n",
            " + dask==2025.2.0\n",
            " - dask-cuda==25.2.0\n",
            " + dask-cuda==25.4.0\n",
            " - dask-cudf-cu12==25.2.2\n",
            " + dask-cudf-cu12==25.4.0\n",
            " + datashader==0.18.1\n",
            " - distributed==2024.12.1\n",
            " + distributed==2025.2.0\n",
            " - distributed-ucxx-cu12==0.42.0\n",
            " + distributed-ucxx-cu12==0.43.0\n",
            " + fqdn==1.5.1\n",
            " + isoduration==20.11.0\n",
            " - jupyter-client==6.1.12\n",
            " + jupyter-client==8.6.3\n",
            " + jupyter-events==0.12.0\n",
            " - jupyter-server==1.16.0\n",
            " + jupyter-server==2.16.0\n",
            " + jupyter-server-proxy==4.4.0\n",
            " + jupyter-server-terminals==0.5.3\n",
            " - libcudf-cu12==25.2.1 (from https://pypi.nvidia.com/libcudf-cu12/libcudf_cu12-25.2.1-py3-none-manylinux_2_28_x86_64.whl)\n",
            " + libcudf-cu12==25.4.0\n",
            " - libcugraph-cu12==25.2.0\n",
            " + libcugraph-cu12==25.4.1\n",
            " - libcuml-cu12==25.2.1\n",
            " + libcuml-cu12==25.4.0\n",
            " + libcuspatial-cu12==25.4.0\n",
            " - libcuvs-cu12==25.2.1\n",
            " + libcuvs-cu12==25.4.0\n",
            " - libkvikio-cu12==25.2.1\n",
            " + libkvikio-cu12==25.4.0\n",
            " - libraft-cu12==25.2.0\n",
            " + libraft-cu12==25.4.0\n",
            " + librmm-cu12==25.4.0\n",
            " - libucxx-cu12==0.42.0\n",
            " + libucxx-cu12==0.43.0\n",
            " - numba-cuda==0.2.0\n",
            " + numba-cuda==0.4.0\n",
            " - nx-cugraph-cu12==25.2.0 (from https://pypi.nvidia.com/nx-cugraph-cu12/nx_cugraph_cu12-25.2.0-py3-none-any.whl)\n",
            " + nx-cugraph-cu12==25.4.0\n",
            " + overrides==7.7.0\n",
            " + pyct==0.5.0\n",
            " - pylibcudf-cu12==25.2.1 (from https://pypi.nvidia.com/pylibcudf-cu12/pylibcudf_cu12-25.2.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl)\n",
            " + pylibcudf-cu12==25.4.0\n",
            " - pylibcugraph-cu12==25.2.0\n",
            " + pylibcugraph-cu12==25.4.1\n",
            " - pylibraft-cu12==25.2.0\n",
            " + pylibraft-cu12==25.4.0\n",
            " + python-json-logger==3.3.0\n",
            " - raft-dask-cu12==25.2.0\n",
            " + raft-dask-cu12==25.4.0\n",
            " - rapids-dask-dependency==25.2.0\n",
            " + rapids-dask-dependency==25.4.0\n",
            " + rapids-logger==0.1.1\n",
            " + rfc3339-validator==0.1.4\n",
            " + rfc3986-validator==0.1.1\n",
            " - rmm-cu12==25.2.0\n",
            " + rmm-cu12==25.4.0\n",
            " - shapely==2.1.1\n",
            " + shapely==2.0.7\n",
            " + simpervisor==1.0.0\n",
            " + types-python-dateutil==2.9.0.20250516\n",
            " - ucx-py-cu12==0.42.0\n",
            " + ucx-py-cu12==0.43.0\n",
            " - ucxx-cu12==0.42.0\n",
            " + ucxx-cu12==0.43.0\n",
            " + uri-template==1.3.0\n",
            "\n",
            "        ***********************************************************************\n",
            "        The pip install of RAPIDS is complete.\n",
            "\n",
            "        Please do not run any further installation from the conda based installation methods, as they may cause issues!\n",
            "\n",
            "        Please ensure that you're pulling from the git repo to remain updated with the latest working install scripts.\n",
            "\n",
            "        Troubleshooting:\n",
            "            - If there is an installation failure, please check back on RAPIDSAI owned templates/notebooks to see how to update your personal files.\n",
            "            - If an installation failure persists when using the latest script, please make an issue on https://github.com/rapidsai-community/rapidsai-csp-utils\n",
            "        ***********************************************************************\n",
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZJMJ6BulmMn"
      },
      "source": [
        "# RAPIDS is now installed on Colab.  \n",
        "You can copy your code into the cells below or use the below to validate your RAPIDS installation and version.  \n",
        "# Enjoy!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nLrk46BllED",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ac10d2b2-d4aa-419d-cb0e-14b7c41c81bb"
      },
      "source": [
        "import cudf\n",
        "cudf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'25.04.00'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cuml\n",
        "cuml.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "xgAFgI15ddf6",
        "outputId": "54a13ac5-0c3b-49d5-b489-cfd6c6eabf37"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'25.04.00'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cugraph\n",
        "cugraph.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JOCMWaUal1fI",
        "outputId": "a0341787-e9e4-4fd5-b454-127e92b9c46c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'24.04.00'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cuspatial\n",
        "cuspatial.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AnmtYjzvVTtv",
        "outputId": "cfc0e3e8-1749-472a-e529-349d78f7eb9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'24.04.00'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cuxfilter\n",
        "cuxfilter.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CYjcARDFVWWD",
        "outputId": "e385e426-e997-4811-9f18-8a9d5dd6dade"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'24.04.01'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dlsyk9m9NN2K"
      },
      "source": [
        "# Next Steps #\n",
        "\n",
        "For an overview of how you can access and work with your own datasets in Colab, check out [this guide](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92).\n",
        "\n",
        "For more RAPIDS examples, check out our RAPIDS notebooks repos:\n",
        "1. https://github.com/rapidsai/notebooks\n",
        "2. https://github.com/rapidsai/notebooks-contrib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "9GE3Jvj8d3_M",
        "outputId": "f533684c-19e7-46a6-e186-7efe93cb50d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cuml\n",
        "from cuml.cluster import HDBSCAN\n",
        "import h5py\n",
        "import numpy as np\n",
        "\n",
        "# Load embeddings from mounted drive\n",
        "with h5py.File('/content/drive/MyDrive/large_scale_embeddings.h5', 'r') as f:\n",
        "    embeddings = f['embeddings'][:]\n",
        "    conversation_ids = [cid.decode('utf-8') for cid in f['conversation_ids'][:]]\n",
        "\n",
        "print(f\"Loaded {len(embeddings):,} embeddings\")"
      ],
      "metadata": {
        "id": "dVxk9W-VGz-u",
        "outputId": "29892534-01d4-4bbd-bc60-5f26e68b58d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 563,166 embeddings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit cuML HDBSCAN\n",
        "clusterer = HDBSCAN(\n",
        "    min_cluster_size=100,\n",
        "    min_samples=25,\n",
        "    cluster_selection_epsilon=0.0,\n",
        "    metric='euclidean',\n",
        "    prediction_data=True,\n",
        "    compute_core_distances=True  # For exemplars\n",
        ")"
      ],
      "metadata": {
        "id": "6EzKvhBxHw9v",
        "outputId": "fa11d0fc-cf6e-47b7-9d5b-2c25916d43de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-05-30 03:12:42.253] [CUML] [info] Unused keyword parameter: compute_core_distances during cuML estimator initialization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Fitting HDBSCAN...\")\n",
        "clusterer.fit(embeddings)\n",
        "\n",
        "print(f\"Found {len(np.unique(clusterer.labels_))-1} clusters\")"
      ],
      "metadata": {
        "id": "LwLu7kz2I8wv",
        "outputId": "fbd8d7c7-99f6-4190-8b36-223b181eee85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting HDBSCAN...\n",
            "Found 400 clusters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import h5py\n",
        "import pickle\n",
        "import json\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "ef convert_numpy_types_for_json(obj):\n",
        "    \"\"\"\n",
        "    Convert numpy types to Python native types for JSON serialization\n",
        "    Based on: https://www.geeksforgeeks.org/fix-type-error-numpy-array-is-not-json-serializable/\n",
        "    \"\"\"\n",
        "    if isinstance(obj, np.integer):\n",
        "        return int(obj)\n",
        "    elif isinstance(obj, np.floating):\n",
        "        return float(obj)\n",
        "    elif isinstance(obj, np.ndarray):\n",
        "        return obj.tolist()\n",
        "    elif isinstance(obj, dict):\n",
        "        return {key: convert_numpy_types_for_json(value) for key, value in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [convert_numpy_types_for_json(item) for item in obj]\n",
        "    elif isinstance(obj, tuple):\n",
        "        return tuple(convert_numpy_types_for_json(item) for item in obj)\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "def save_cuml_hdbscan_results(clusterer, embeddings, conversation_ids, output_dir=\"hdbscan_results\"):\n",
        "    \"\"\"\n",
        "    Save all necessary results from cuML HDBSCAN for local processing\n",
        "\n",
        "    Args:\n",
        "        clusterer: Fitted cuML HDBSCAN object\n",
        "        embeddings: Original embeddings array\n",
        "        conversation_ids: List of conversation IDs\n",
        "        output_dir: Directory to save results\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"üíæ Saving cuML HDBSCAN results for local import...\")\n",
        "    Path(output_dir).mkdir(exist_ok=True)\n",
        "\n",
        "    # 1. CORE CLUSTERING RESULTS\n",
        "    print(\"üìä Saving core clustering results...\")\n",
        "\n",
        "    # Handle cuML-specific objects that may not have .copy() method\n",
        "    def safe_copy_or_convert(obj, attr_name):\n",
        "        \"\"\"Safely copy or convert cuML objects to serializable format\"\"\"\n",
        "        if not hasattr(clusterer, attr_name):\n",
        "            return None\n",
        "\n",
        "        attr = getattr(clusterer, attr_name)\n",
        "        if attr is None:\n",
        "            return None\n",
        "\n",
        "        # Try to copy first\n",
        "        if hasattr(attr, 'copy'):\n",
        "            try:\n",
        "                return attr.copy()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Try to convert to numpy array\n",
        "        if hasattr(attr, 'to_numpy'):\n",
        "            try:\n",
        "                return attr.to_numpy()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # Try to convert to pandas DataFrame then to dict\n",
        "        if hasattr(attr, 'to_pandas'):\n",
        "            try:\n",
        "                df = attr.to_pandas()\n",
        "                return df.to_dict()\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # For CondensedTree objects, try to extract meaningful data\n",
        "        if hasattr(attr, '__dict__'):\n",
        "            try:\n",
        "                # Try to serialize the object's attributes\n",
        "                result = {}\n",
        "                for key, value in attr.__dict__.items():\n",
        "                    if isinstance(value, (int, float, str, bool, type(None))):\n",
        "                        result[key] = value\n",
        "                    elif hasattr(value, 'tolist'):  # numpy arrays\n",
        "                        result[key] = value.tolist()\n",
        "                return result\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        # If all else fails, return a placeholder\n",
        "        print(f\"‚ö†Ô∏è Could not serialize {attr_name}, skipping...\")\n",
        "        return f\"cuml_object_{attr_name}_not_serializable\"\n",
        "\n",
        "    core_results = {\n",
        "        # Essential cluster assignments\n",
        "        'labels_': clusterer.labels_.copy() if hasattr(clusterer.labels_, 'copy') else clusterer.labels_,\n",
        "        'probabilities_': safe_copy_or_convert(clusterer, 'probabilities_'),\n",
        "\n",
        "        # Cluster statistics\n",
        "        'cluster_persistence_': safe_copy_or_convert(clusterer, 'cluster_persistence_'),\n",
        "        'condensed_tree_': safe_copy_or_convert(clusterer, 'condensed_tree_'),\n",
        "        'single_linkage_tree_': safe_copy_or_convert(clusterer, 'single_linkage_tree_'),\n",
        "\n",
        "        # Exemplars and representatives\n",
        "        'exemplars_': safe_copy_or_convert(clusterer, 'exemplars_'),\n",
        "        'outlier_scores_': safe_copy_or_convert(clusterer, 'outlier_scores_'),\n",
        "\n",
        "        # Configuration used\n",
        "        'min_cluster_size': clusterer.min_cluster_size,\n",
        "        'min_samples': clusterer.min_samples,\n",
        "        'cluster_selection_epsilon': clusterer.cluster_selection_epsilon,\n",
        "        'algorithm': clusterer.algorithm if hasattr(clusterer, 'algorithm') else 'auto',\n",
        "        'metric': clusterer.metric if hasattr(clusterer, 'metric') else 'euclidean'\n",
        "    }\n",
        "\n",
        "    # Save core results\n",
        "    with open(f\"{output_dir}/core_results.pkl\", 'wb') as f:\n",
        "        pickle.dump(core_results, f)\n",
        "\n",
        "    # 2. CONVERSATION-TO-CLUSTER MAPPING\n",
        "    print(\"üó∫Ô∏è Saving conversation-to-cluster mapping...\")\n",
        "\n",
        "    conversation_mapping = {\n",
        "        'conversation_ids': conversation_ids,\n",
        "        'cluster_labels': clusterer.labels_.copy() if hasattr(clusterer.labels_, 'copy') else clusterer.labels_,\n",
        "        'soft_probabilities': safe_copy_or_convert(clusterer, 'probabilities_'),\n",
        "        'outlier_scores': safe_copy_or_convert(clusterer, 'outlier_scores_'),\n",
        "        'total_conversations': len(conversation_ids)\n",
        "    }\n",
        "\n",
        "    with open(f\"{output_dir}/conversation_mapping.pkl\", 'wb') as f:\n",
        "        pickle.dump(conversation_mapping, f)\n",
        "\n",
        "    # 3. CLUSTER STATISTICS AND PROTOTYPES\n",
        "    print(\"üìà Computing and saving cluster statistics...\")\n",
        "\n",
        "    unique_labels = np.unique(clusterer.labels_)\n",
        "    valid_clusters = unique_labels[unique_labels != -1]  # Exclude noise\n",
        "\n",
        "    # Cache converted objects for efficiency\n",
        "    outlier_scores = safe_copy_or_convert(clusterer, 'outlier_scores_')\n",
        "    probabilities = safe_copy_or_convert(clusterer, 'probabilities_')\n",
        "    cluster_persistence = safe_copy_or_convert(clusterer, 'cluster_persistence_')\n",
        "\n",
        "    cluster_stats = {}\n",
        "    cluster_prototypes = {}\n",
        "\n",
        "    for cluster_id in valid_clusters:\n",
        "        cluster_mask = clusterer.labels_ == cluster_id\n",
        "        cluster_indices = np.where(cluster_mask)[0]\n",
        "        cluster_embeddings = embeddings[cluster_indices]\n",
        "\n",
        "        # Safely compute statistics\n",
        "        mean_outlier_score = None\n",
        "        if outlier_scores is not None and not isinstance(outlier_scores, str):\n",
        "            try:\n",
        "                mean_outlier_score = float(np.mean(outlier_scores[cluster_indices]))\n",
        "            except:\n",
        "                mean_outlier_score = None\n",
        "\n",
        "        mean_probability = None\n",
        "        if probabilities is not None and not isinstance(probabilities, str):\n",
        "            try:\n",
        "                mean_probability = float(np.mean(probabilities[cluster_indices]))\n",
        "            except:\n",
        "                mean_probability = None\n",
        "\n",
        "        persistence_score = None\n",
        "        if cluster_persistence is not None and not isinstance(cluster_persistence, str):\n",
        "            try:\n",
        "                if cluster_id < len(cluster_persistence):\n",
        "                    persistence_score = float(cluster_persistence[cluster_id])\n",
        "            except:\n",
        "                persistence_score = None\n",
        "\n",
        "        # Basic statistics\n",
        "        cluster_stats[int(cluster_id)] = {\n",
        "            'size': len(cluster_indices),\n",
        "            'conversation_indices': cluster_indices.tolist(),\n",
        "            'mean_outlier_score': mean_outlier_score,\n",
        "            'mean_probability': mean_probability,\n",
        "            'persistence': persistence_score\n",
        "        }\n",
        "\n",
        "        # Prototype selection (multiple methods for robustness)\n",
        "        prototypes = select_cluster_prototypes(\n",
        "            cluster_embeddings,\n",
        "            cluster_indices,\n",
        "            conversation_ids,\n",
        "            max_prototypes=15\n",
        "        )\n",
        "\n",
        "        cluster_prototypes[int(cluster_id)] = prototypes\n",
        "\n",
        "    # Save cluster analysis\n",
        "    with open(f\"{output_dir}/cluster_statistics.pkl\", 'wb') as f:\n",
        "        pickle.dump(cluster_stats, f)\n",
        "\n",
        "    with open(f\"{output_dir}/cluster_prototypes.pkl\", 'wb') as f:\n",
        "        pickle.dump(cluster_prototypes, f)\n",
        "\n",
        "    # 4. SOFT CLUSTERING RESULTS (if available)\n",
        "    if probabilities is not None and not isinstance(probabilities, str):\n",
        "        print(\"üéØ Saving soft clustering results...\")\n",
        "\n",
        "        try:\n",
        "            # Calculate uncertainty scores safely\n",
        "            if probabilities.ndim > 1:\n",
        "                uncertainty_scores = 1 - np.max(probabilities, axis=1)\n",
        "            else:\n",
        "                uncertainty_scores = 1 - probabilities\n",
        "\n",
        "            soft_clustering = {\n",
        "                'probabilities': probabilities.tolist() if hasattr(probabilities, 'tolist') else probabilities,\n",
        "                'membership_vectors': get_soft_membership_vectors(clusterer.labels_, probabilities),\n",
        "                'uncertainty_scores': uncertainty_scores.tolist() if hasattr(uncertainty_scores, 'tolist') else uncertainty_scores\n",
        "            }\n",
        "\n",
        "            with open(f\"{output_dir}/soft_clustering.pkl\", 'wb') as f:\n",
        "                pickle.dump(soft_clustering, f)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not save soft clustering results: {e}\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è No soft clustering probabilities available\")\n",
        "\n",
        "    # 5. HIERARCHY INFORMATION (if available)\n",
        "    condensed_tree = safe_copy_or_convert(clusterer, 'condensed_tree_')\n",
        "    single_linkage_tree = safe_copy_or_convert(clusterer, 'single_linkage_tree_')\n",
        "\n",
        "    if condensed_tree is not None and not isinstance(condensed_tree, str):\n",
        "        print(\"üå≥ Saving cluster hierarchy...\")\n",
        "\n",
        "        try:\n",
        "            hierarchy_info = {\n",
        "                'condensed_tree': condensed_tree,\n",
        "                'single_linkage_tree': single_linkage_tree if single_linkage_tree is not None and not isinstance(single_linkage_tree, str) else None,\n",
        "                'cluster_hierarchy': extract_cluster_hierarchy(clusterer)\n",
        "            }\n",
        "\n",
        "            with open(f\"{output_dir}/hierarchy_info.pkl\", 'wb') as f:\n",
        "                pickle.dump(hierarchy_info, f)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Could not save hierarchy information: {e}\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è No hierarchy information available\")\n",
        "\n",
        "    # 6. SUMMARY METADATA\n",
        "    print(\"üìã Saving summary metadata...\")\n",
        "\n",
        "    summary = {\n",
        "        'timestamp': time.time(),\n",
        "        'total_conversations': len(conversation_ids),\n",
        "        'total_clusters': len(valid_clusters),\n",
        "        'noise_points': int(np.sum(clusterer.labels_ == -1)),\n",
        "        'noise_percentage': float(np.sum(clusterer.labels_ == -1) / len(clusterer.labels_) * 100),\n",
        "        'largest_cluster_size': int(max([stats['size'] for stats in cluster_stats.values()])) if cluster_stats else 0,\n",
        "        'smallest_cluster_size': int(min([stats['size'] for stats in cluster_stats.values()])) if cluster_stats else 0,\n",
        "        'mean_cluster_size': float(np.mean([stats['size'] for stats in cluster_stats.values()])) if cluster_stats else 0.0,\n",
        "        'has_soft_clustering': probabilities is not None and not isinstance(probabilities, str),\n",
        "        'has_hierarchy': condensed_tree is not None and not isinstance(condensed_tree, str),\n",
        "        'algorithm_params': {\n",
        "            'min_cluster_size': clusterer.min_cluster_size,\n",
        "            'min_samples': clusterer.min_samples,\n",
        "            'cluster_selection_epsilon': clusterer.cluster_selection_epsilon\n",
        "        }\n",
        "    }\n",
        "\n",
        "    with open(f\"{output_dir}/summary.json\", 'w') as f:\n",
        "        json.dump(convert_numpy_types_for_json(summary), f, indent=2)\n",
        "\n",
        "    print(\"‚úÖ All cuML HDBSCAN results saved successfully!\")\n",
        "    print(f\"üìÅ Results saved to: {output_dir}/\")\n",
        "    print(f\"üìä {len(valid_clusters)} clusters, {np.sum(clusterer.labels_ == -1)} noise points\")\n",
        "\n",
        "    return output_dir\n",
        "\n",
        "def select_cluster_prototypes(cluster_embeddings, cluster_indices, conversation_ids, max_prototypes=15):\n",
        "    \"\"\"Select diverse prototypes from a cluster using multiple methods\"\"\"\n",
        "\n",
        "    if len(cluster_embeddings) <= max_prototypes:\n",
        "        return {\n",
        "            'all_indices': cluster_indices.tolist(),\n",
        "            'all_conversation_ids': [conversation_ids[i] for i in cluster_indices],\n",
        "            'selection_method': 'all_points'\n",
        "        }\n",
        "\n",
        "    # Method 1: Centroid-nearest\n",
        "    centroid = np.mean(cluster_embeddings, axis=0)\n",
        "    distances_to_centroid = np.linalg.norm(cluster_embeddings - centroid, axis=1)\n",
        "    centroid_nearest_idx = np.argsort(distances_to_centroid)[:max_prototypes//3]\n",
        "\n",
        "    # Method 2: Diverse exemplars (farthest-first traversal)\n",
        "    diverse_idx = farthest_first_traversal(cluster_embeddings, max_prototypes//3)\n",
        "\n",
        "    # Method 3: Boundary cases (points with highest variance from centroid)\n",
        "    boundary_idx = np.argsort(distances_to_centroid)[-max_prototypes//3:]\n",
        "\n",
        "    # Combine and deduplicate\n",
        "    all_prototype_idx = np.unique(np.concatenate([centroid_nearest_idx, diverse_idx, boundary_idx]))\n",
        "\n",
        "    # Convert to original indices\n",
        "    prototype_indices = cluster_indices[all_prototype_idx]\n",
        "\n",
        "    return {\n",
        "        'prototype_indices': prototype_indices.tolist(),\n",
        "        'prototype_conversation_ids': [conversation_ids[i] for i in prototype_indices],\n",
        "        'centroid_nearest': cluster_indices[centroid_nearest_idx].tolist(),\n",
        "        'diverse_exemplars': cluster_indices[diverse_idx].tolist(),\n",
        "        'boundary_cases': cluster_indices[boundary_idx].tolist(),\n",
        "        'selection_methods': ['centroid_nearest', 'diverse_exemplars', 'boundary_cases']\n",
        "    }\n",
        "\n",
        "def farthest_first_traversal(embeddings, n_exemplars):\n",
        "    \"\"\"Select diverse exemplars using farthest-first traversal\"\"\"\n",
        "    if n_exemplars >= len(embeddings):\n",
        "        return np.arange(len(embeddings))\n",
        "\n",
        "    selected = [0]  # Start with first point\n",
        "\n",
        "    for _ in range(n_exemplars - 1):\n",
        "        max_min_distance = -1\n",
        "        best_candidate = -1\n",
        "\n",
        "        for candidate in range(len(embeddings)):\n",
        "            if candidate in selected:\n",
        "                continue\n",
        "\n",
        "            # Find minimum distance to any selected point\n",
        "            min_distance = float('inf')\n",
        "            for selected_idx in selected:\n",
        "                distance = np.linalg.norm(embeddings[candidate] - embeddings[selected_idx])\n",
        "                min_distance = min(min_distance, distance)\n",
        "\n",
        "            # Update best candidate if this has larger minimum distance\n",
        "            if min_distance > max_min_distance:\n",
        "                max_min_distance = min_distance\n",
        "                best_candidate = candidate\n",
        "\n",
        "        if best_candidate != -1:\n",
        "            selected.append(best_candidate)\n",
        "\n",
        "    return np.array(selected)\n",
        "\n",
        "def get_soft_membership_vectors(labels, probabilities):\n",
        "    \"\"\"Extract soft membership vectors for each cluster\"\"\"\n",
        "    unique_labels = np.unique(labels)\n",
        "    valid_clusters = unique_labels[unique_labels != -1]\n",
        "\n",
        "    if probabilities.ndim == 1:\n",
        "        # Single probability per point\n",
        "        membership = {}\n",
        "        for cluster_id in valid_clusters:\n",
        "            cluster_mask = labels == cluster_id\n",
        "            membership[int(cluster_id)] = probabilities[cluster_mask].tolist()\n",
        "        return membership\n",
        "    else:\n",
        "        # Probability matrix\n",
        "        membership = {}\n",
        "        for i, cluster_id in enumerate(valid_clusters):\n",
        "            membership[int(cluster_id)] = probabilities[:, i].tolist()\n",
        "        return membership\n",
        "\n",
        "def extract_cluster_hierarchy(clusterer):\n",
        "    \"\"\"Extract interpretable cluster hierarchy information\"\"\"\n",
        "    if not hasattr(clusterer, 'condensed_tree_'):\n",
        "        return None\n",
        "\n",
        "    # This would need to be customized based on cuML's specific tree structure\n",
        "    # For now, return basic information\n",
        "    return {\n",
        "        'tree_structure': 'condensed_tree_available',\n",
        "        'note': 'Detailed hierarchy extraction depends on cuML tree format'\n",
        "    }"
      ],
      "metadata": {
        "id": "PwIzyf1bJurW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = save_cuml_hdbscan_results(\n",
        "    clusterer,\n",
        "    embeddings,\n",
        "    conversation_ids,\n",
        "    output_dir=\"/content/drive/MyDrive/hdbscan_results\"\n",
        ")"
      ],
      "metadata": {
        "id": "7h0zF5a8KVPo",
        "outputId": "c13c283a-c3d9-431f-993f-4c40b3157c75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üíæ Saving cuML HDBSCAN results for local import...\n",
            "üìä Saving core clustering results...\n",
            "üó∫Ô∏è Saving conversation-to-cluster mapping...\n",
            "üìà Computing and saving cluster statistics...\n",
            "üéØ Saving soft clustering results...\n",
            "üå≥ Saving cluster hierarchy...\n",
            "üìã Saving summary metadata...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Object of type int64 is not JSON serializable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-809957a205ff>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m output_dir = save_cuml_hdbscan_results(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mclusterer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mconversation_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/hdbscan_results\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-2bfdc0ee0e67>\u001b[0m in \u001b[0;36msave_cuml_hdbscan_results\u001b[0;34m(clusterer, embeddings, conversation_ids, output_dir)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{output_dir}/summary.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"‚úÖ All cuML HDBSCAN results saved successfully!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/__init__.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;31m# could accelerate with writelines in some versions of Python, at\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# a debuggability cost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    404\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    437\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    181\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Object of type int64 is not JSON serializable"
          ]
        }
      ]
    }
  ]
}